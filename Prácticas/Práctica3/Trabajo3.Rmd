---
title: "Trabajo 3"
author: "Luis Suárez Lloréns"
date: "4 de mayo de 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(5552368)
library(ROCR)
```

## Apartado 1

Antes de empezar a responder a los diferentes puntos del apartado, debemos cargar los datos de la base de datos *Auto*. Estos se encuentran en la librería *ISLR* de R. Cargamos la librería con la siguiente orden.
```{r Carga.ISLR}
library(ISLR)
```

### a)

Vamos a usar la función *pairs* para tener una visión general de los datos.
```{r}
pairs(Auto)
```

Nos fijamos en la fila de mpg, que es la característica que queremos clasificar. Viendo las gráficas de la fila de mpg, podemos ver que hay 3 que muestran cierta tendencia, que son *displacement*, *horsepower* y *weight*. Las demás muestran nubes de puntos demasiado amplias y difuminadas, que no pueden ser ajustadas.

Vamos a usar la función *boxplot* para representar estas tres gráficas.

**DISPLACEMENT**
```{r}
boxplot(mpg~cut(displacement, breaks = 10),data = Auto)
```

**HORSEPOWER**
```{r}
boxplot(mpg~cut(horsepower, breaks = 10),data = Auto)
```

**WEIGHT**
```{r}
boxplot(mpg~cut(weight, breaks = 10),data = Auto)
```

De las anteriores, el dato *displacement* sufre un repunte, en el intervalo (262,300]. Si nos fijamos realmente en los datos, esta sección tiene muy pocos datos, es normal que se pueda ver alterado. Salvo esto, las 3 muestran una tendencia clara. Si hubiera que elegir entre una de las tres, probablemente cogería *horsepower*, pues *weight* tiene muchos outliners y *displacement* tiene ese comportamiento un poco extraño comentado antes.

### b)

Por lo dicho anteriormente, seleccionaremos como posibles variables predictoras *displacement*, *horsepower* y *weight*, pues las nubes de puntos parecen reflejar una tendencia clara.

### c)

Para separar los datos en entrenamiento y test, vamos a usar un muestreo sin remplazamiento. En nuestro caso, como tenemos muchos datos, el comportamiento de este muestreo aleatorio debería cubrir bien todos los casos. En el caso de que no se tuvieran buenos resultados, sería necesario hacer este muestreo de manera estratificada.

```{r}
idx.train = sample(nrow(Auto),size = nrow(Auto)*0.8)
Auto.train = Auto[idx.train,c("mpg","displacement","horsepower","weight")]
Auto.test = Auto[-idx.train,c("mpg","displacement","horsepower","weight")]
```

### d)

Vamos a crear la variable mpg01. Para poder hacer el aprendizaje de modo correcto, la mediana la realizaremos sólo con los datos de entrenamiento, y dicha mediana la usaremos para ambos conjuntos.

```{r}
Auto.train = data.frame(mpg01=sign(Auto.train$mpg>=median(Auto.train$mpg))*2-1, Auto.train)
Auto.test = data.frame(mpg01=sign(Auto.test$mpg>=median(Auto.train$mpg))*2-1, Auto.test)
```

Una vez tenemos los datos formateados de la manera desada, procedemos a aplicar los distintos modelos que se nos piden.

#### Regresión logística

Para realizar la regresión logística, utilizamos la función *glm*.

```{r}
modelo.RegLog = glm(mpg01 ~ displacement + horsepower + weight, data = Auto.train, start=c(log(mean(Auto.train$mpg)),0,0,0))
```

Con el modelo ya aprendido, realizamos la predicción del test.

```{r}
prediccion.glm = predict(modelo.RegLog,newdata = Auto.test)
```

Ahora, definimos como error cuando el clasificador nos daría la clase incorrecta. Esto se produce cuando el signo de la predicción y el de *mpg01* son distintos.

```{r}
sum((sign(-prediccion.glm*Auto.test$mpg01)+1)/2)/nrow(Auto.test) * 100
```

Obtenemos un error bastante bajo, del 6 por ciento.

#### K-NN

Hacemos algo similar que lo realizado para la regresión logística, pero con la función *knn*.

```{r}
library(class)
prediccion.knn = knn(Auto.train[,c("displacement","horsepower","weight")],Auto.test[,c("displacement","horsepower","weight")], Auto.train[,"mpg01"],k = 3)

```

A diferencia de la regresión, los resultados del clasificador K-NN son directamente las clases. Luego para saber si hemos fallado, sólo tenemos que mirar si los resultados son distintos.
```{r}
sum(prediccion.knn != Auto.test$mpg01)/nrow(Auto.test) * 100
```

El error que encontramos está entorno al 8 por ciento.

Para intentar conocer el mejor parámetro de la variable *k*, vamos a realizar la prueba anterior cambiando los valores de k.

```{r}
for(k in 1:20){
  prediccion.knn = knn(Auto.train[,c("displacement","horsepower","weight")],Auto.test[,c("displacement","horsepower","weight")], Auto.train[,"mpg01"],k = k)
  error = sum(prediccion.knn != Auto.test$mpg01)/nrow(Auto.test) * 100
  cat(paste0("K: ",k, " \tError: ",error,"\n"))
}
```

Podemos ver, los valores que tienen menos error son 5 y 7. No tenemos más información, luego no podemos decidirnos entre los dos. 

R tiene una funcionalidad que, automáticamente nos devuelve el mejor parámetro *k*. Es la función *tune.knn()* de la librería *e1071*.
```{r warning=FALSE}
library(e1071)
tune.values = tune.knn(x = Auto.train[,c("displacement","horsepower","weight")], y = as.factor(Auto.train[,"mpg01"]),k = 1:20,tunecontrol = tune.control(sampling = "cross"), cross = 10)
summary(tune.values)
```

Para poder realizar el ejercicio sobre la curva ROC, guardamos la predicción del K-NN, pero obteniendo la probabilidad de pertenencia a la clase, y no sólo la clase.

```{r}
prediccion.knn = knn(Auto.train[,c("displacement","horsepower","weight")],Auto.test[,c("displacement","horsepower","weight")], Auto.train[,"mpg01"],k = 5, prob = TRUE)

prob <- attr(prediccion.knn, "prob")
prob <- 2*ifelse(prediccion.knn == "-1", 1-prob, prob) - 1
```


#### Curvas ROC
Para poder pintar las *curvas ROC*, tenemos que usar las ordenes del paquete *ROCR* *prediction* y *performance*.

Primero, pintamos la curva de la regresión logística.
```{r}
pred <- prediction(prediccion.glm, Auto.test$mpg01)
perf <- performance(pred, measure = "tpr", x.measure = "fpr") 
plot(perf, col=rainbow(10))
```

Después, pintamos la curva del clasificador K-NN
```{r}
pred <- prediction(prob, Auto.test$mpg01)
perf <- performance(pred, measure = "tpr", x.measure = "fpr") 
plot(perf, col=rainbow(10))
```

Podemos ver que tenemos curvas similares en ambos casos. Destacar que parece que el K-NN trabaja mejor con pocos falsos positivos, lo que nos puede llevar a elegirlo sobre la regresión logística en algunos casos.

### e) (Bonus)

Para poder estimar los dos modelos usando validación cruzada, tenemos que generar las 5 particiones. Para crear los *folds* o particiones que usaremos para la validación cruzada, podemos usar la función *createFolds* del paquete *caret*.
```{r warning=FALSE}
library(caret)
folds = createFolds(1:nrow(Auto), k = 5)

error.log = vector("numeric",5)
error.knn = vector("numeric",5)

for(i in 1:5){
  Auto.train = Auto[-folds[[i]],c("mpg","displacement","horsepower","weight")]
  Auto.test = Auto[folds[[i]],c("mpg","displacement","horsepower","weight")]
  
  Auto.train = data.frame(mpg01=sign(Auto.train$mpg>=median(Auto.train$mpg))*2-1, Auto.train)
  Auto.test = data.frame(mpg01=sign(Auto.test$mpg>=median(Auto.train$mpg))*2-1, Auto.test)
  
  #Regresion logistica
  prediccion.glm = predict(modelo.RegLog,newdata = Auto.test)
  error.log[i] = sum((sign(-prediccion.glm*Auto.test$mpg01)+1)/2)/nrow(Auto.test) * 100
  #Clasificacion K-NN
  prediccion.knn = knn(Auto.train[,c("displacement","horsepower","weight")],Auto.test[,c("displacement","horsepower","weight")], Auto.train[,"mpg01"],k = 5)
  error.knn[i] = sum(prediccion.knn != Auto.test$mpg01)/nrow(Auto.test) * 100
}

cat(paste0("Error medio cometido con regresión logística: ",mean(error.log)))
cat(paste0("Error medio cometido con K-NN: ",mean(error.knn)))
```


Podemos por tanto, confirmar lo visto en el apartado anterior. La regresión logística nos ofrece un error menor que K-NN.

## Apartado 2

Primero, vamos a cargar los datos, y a realizar una partición de los mismos.

```{r}
library(MASS)
idx.train = sample(nrow(Boston),size = nrow(Boston)*0.8)
Boston.train = Boston[idx.train,]
Boston.test = Boston[-idx.train,]
```

Vamos a usar un ajuste de tipo LASSO.

```{r warning=FALSE}
library(glmnet)
prediccion.lasso <- glmnet(x = as.matrix(Boston.train[,-1]), y = Boston.train[,1])
cv = cv.glmnet(x = as.matrix(Boston.train[,-1]), y = Boston.train[,1], alpha = 1)
```

Ahora, tomamos los coeficientes y vemos los que están por encima de un umbral dado. En este caso hemos escogido 0.5.

```{r}
lasso.coef = predict(prediccion.lasso,type="coefficients", s = cv$lambda.min)[1:14,]
coef = abs(lasso.coef)>0.5
coef
Boston.train[1,coef]
```

Por tanto, vemos que las variables que vamos a usar son *chas*, *nox*, *rm*, *dis* y *rad*.

### b)

Ahora entrenamos el modelo con *weight-decay* que nos pide el ejercicio con las variables que conseguimos en el apartado anterior.

```{r}
wd = glmnet(x = as.matrix(Boston.train[,coef]), y = Boston.train[,1], alpha = 0)
```

Con el modelo ya entrenado, vamos a predecir los datos del test, y a ver el error cometido. 

```{r}
prediction = predict(wd, newx = as.matrix(Boston.test[,coef]),s = cv$lambda.min, type = "response")
error = mean((prediction - Boston.test[,1])^2)
cat(paste0("Error cometido: ",error))
```

El error que obtenemos es bastante bueno, por tanto, en un principio no hay problema de *underfitting*.

### c)

Vamos a generar la variable adicional.

```{r}
crim01 = sign(Boston[,1] >= median(Boston[,1]))*2-1
new.crim = crim01[idx.train]
Boston.train.01 = data.frame(Boston.train, new.crim)
new.crim = crim01[-idx.train]
Boston.test.01  = data.frame(Boston.test, new.crim)
```

Ahora, una vez tenemos generados las nuevas variables, vamos a entrenar el SVM con núcleo lineal.

```{r}
model.svm = svm(new.crim ~ chas+nox+rm+dis+rad, data = Boston.train.01, kernel = "linear")
prediction = predict(model.svm, newdata = Boston.test.01)

(sum((sign(-prediction*Boston.test.01$new.crim)+1)/2)/nrow(Boston.test.01)) *100
```


Ahora vamos a probar con otros núcleos, para ver si mejoramos el resultado. Primero veamos que pasa con un kernel polinomial.

```{r}
model.svm = svm(new.crim ~ chas+nox+rm+dis+rad, data = Boston.train.01,kernel = "polynomial")
prediction = predict(model.svm, newdata = Boston.test.01)

(sum((sign(-prediction*Boston.test.01$new.crim)+1)/2)/nrow(Boston.test.01)) *100
```

Y ahora con un kernel radial.

```{r}
model.svm = svm(new.crim ~ chas+nox+rm+dis+rad, data = Boston.train.01,kernel = "radial")
prediction = predict(model.svm, newdata = Boston.test.01)

(sum((sign(-prediction*Boston.test.01$new.crim)+1)/2)/nrow(Boston.test.01)) *100
```

Como podemos ver, el kernel radial obtiene los mejores resultados de error en test.

## Apartado 3

### a)
Para empezar, vamos a prepara las particiones de entrenamiento(80%) y de test(20%), como ya hemos realizado en otros apartados.

```{r}
library(MASS)
idx.train = sample(nrow(Boston),size = nrow(Boston)*0.8)
Boston.train = Boston[idx.train,]
Boston.test = Boston[-idx.train,]
```

Con los datos ya preparamos, vamos a realizar los ejercicios.

### b)
Bagging es un *random forest* donde podemos coger cualquier atributo para la generación de arboles. Por tanto, seleccionamos el parámetro *mtry* que indica el número de atributos a considerar para generar un árbol como el número total de atributos, $13$.

```{r}
library(randomForest)
random.forest = randomForest(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat, data = Boston.train, mtry = 13)
pred = predict(random.forest,Boston.test)
error = sum(abs(Boston.test["medv"]-pred))/nrow(Boston.test)
cat(paste0("El error obtenido es: ",error))
```

Obtenemos un error bastante bajo, entorno al 2%.

### c)

Para un *random forest* general, eliminamos el parámetro *mtry* usado en el apartado anterior. Entonces automáticamente, la función tomará un número menor de atributos que el máximo, teniendo en cuenta el número total de atributos. En resumen, eligiremos entre menos atributos que el total, realizando por tanto un *random forest*.

```{r}
random.forest = randomForest(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat, data = Boston.train)
pred = predict(random.forest,Boston.test)
error = sum(abs(Boston.test["medv"]-pred))/nrow(Boston.test)
cat(paste0("El error obtenido es: ",error))
```

El error obtenido es ligeramente inferior al bagging, pero casi no se puede apreciar.

### d)

Vamos a ajustar un modelo de Boosting para poder comparar los resultados de los apartados anteriores.

```{r}
library(gbm)
boost = gbm(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat, data = Boston.train, distribution = "gaussian")
pred = predict(boost,Boston.test,n.trees = 100)
error = sum(abs(Boston.test["medv"]-pred))/nrow(Boston.test)
cat(paste0("El error obtenido es: ",error))
```

Este error es entorno al 6%. Es decir, que ambos modelos, tanto *bagging* como *random forest* son muy buenas estimaciones, sacando gran ventaja al modelo de *Boosting*. Podemos ver entonces el potencial de generar la decisión tomando multiples opiniones (multiples árboles) obtiene muy buenos resultados, ligeramente mejores para *random forest*.

## Apartado 4

### a)

Primero, vamos a preparar los datos. Vamos a utilizar el mismo mecanismo que en los demás apartados.

```{r}
idx.train = sample(nrow(OJ),size = 800)
OJ.train = OJ[idx.train,]
OJ.test = OJ[-idx.train,]
```

Vamos a entrenar el árbol de decisión.

```{r warning=FALSE}
library(tree)
arbol = tree(Purchase~WeekofPurchase+StoreID+PriceCH+PriceMM+DiscCH+DiscMM+SpecialCH+SpecialMM+LoyalCH+SalePriceMM+SalePriceCH+PriceDiff+Store7+PctDiscMM+PctDiscCH+ListPriceDiff+STORE, data = OJ.train)
```

Con esto, ya habríamos ajustado nuestro árbol con los datos de entrenamiento.

### b)

Vamos a ver los resultados de la orden *summary*.

```{r}
summary(arbol)
```

Obtenemos una lista de características del árbol que hemos generado. Destacar primero que pese al gran número de variables, sólo usamos 5 para el árbol final. Esto podría ser sorprendente, pues dado la gran cantidad de variables que podría seleccionar, selecciona un grupo muy pequeño. Debido el funcionamiento de los árboles, si usaramos todas las variables, el árbol sería mucho más preciso, pudiendo fácilmente legar el error a 0. Lo que sucede es que se somete al árbol a un proceso de regularización para mejorar la generalización, que nos reduce el árbol hasta lo que obtenemos.

Destacar también el error que nos da, que es entorno a $0.15$. 

### c)

```{r}
plot(arbol, uniform=TRUE, 
  	main="Árbol de clasificación")
text(arbol, use.n=TRUE, all=TRUE, cex=.8)
```

Podemos ver que el árbol cumple los datos que obteníamos del *summary*. Las primeras elecciones, como sabemos por el funcionamiento de la generación de los árboles, son las que aportan más separación a los datos. Entonces, podemos usar esta representación del árbol para conocer los valores más importantes para clasificar la muestra. En este caso, el factor que claramente es más importante es el *LoyalCH*.

### d)

Para poder calcular la matriz de confusión, debemos tomar una decisión sobre el valor donde pasamos de clasificar un punto como *CH* para clasificarlo como *MM*. Por defecto, vamos a tomar la decisión de asignar a los valores superiores de $0.5$ a *CH* y los valores menores o iguales a la clase *MM*

```{r}
probabilidad.prediccion = predict(arbol,OJ.test)
OJ.prediction= vector(length = nrow(probabilidad.prediccion))
OJ.prediction[probabilidad.prediccion[,1]>0.5] = "CH"
OJ.prediction[probabilidad.prediccion[,1]<=0.5] = "MM"
confusionMatrix(OJ.prediction,OJ.test$Purchase)
```

Obtenemos un error de $0.76$. Destacar que la mayor parte del error se produce al intentar clasificar muestras de MM, pues obtenemos muchos falsos *CH*. Por tanto si fueramos más restriccivos con la clasificación de *CH* (subiendo el valor 0.5) deberíamos obtener mejores resultados.

```{r}
probabilidad.prediccion = predict(arbol,OJ.test)
OJ.prediction= vector(length = nrow(probabilidad.prediccion))
OJ.prediction[probabilidad.prediccion[,1]>0.6] = "CH"
OJ.prediction[probabilidad.prediccion[,1]<=0.6] = "MM"
confusionMatrix(OJ.prediction,OJ.test$Purchase)
```

Efectivamente, a costa de empeorar la sensitividad, mejoramos el error general y sobretodo la espificidad. 

### e) y bonus

Vamos a realizar un experimento con validación cruzada, utilizando la función *cv.tree()*

```{r}
tr0.cv = cv.tree(arbol)
plot.tree.sequence(tr0.cv)
```

Como podemos ver, el tamaño del árbol óptimo está entorno a 5. Esto nos indica que el árbol que hemos generado durante el ejercicio tiene demasiados nodos.