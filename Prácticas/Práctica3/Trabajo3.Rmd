---
title: "Trabajo 3"
author: "Luis Suárez Lloréns"
date: "4 de mayo de 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(5552368)
library(ROCR)
```

## Apartado 1

Antes de empezar a responder a los diferentes puntos del apartado, debemos cargar los datos de la base de datos *Auto*. Estos se encuentran en la librería *ISLR* de R. Cargamos la librería con la siguiente orden.
```{r Carga.ISLR}
library(ISLR)
```

### a)

Vamos a usar la función *pairs* para tener una visión general de los datos.
```{r}
pairs(Auto)
```

Nos fijamos en la fila de mpg, que es la característica que queremos clasificar. Viendo las gráficas de la fila de mpg, podemos ver que hay 3 que muestran cierta tendencia, que son *displacement*, *horsepower* y *weight*. Las demás muestran nubes de puntos demasiado amplias y difuminadas, que no pueden ser ajustadas.

Vamos a usar la función *boxplot* para representar estas tres gráficas.

**DISPLACEMENT**
```{r}
boxplot(mpg~cut(displacement, breaks = 10),data = Auto)
```

**HORSEPOWER**
```{r}
boxplot(mpg~cut(horsepower, breaks = 10),data = Auto)
```

**WEIGHT**
```{r}
boxplot(mpg~cut(weight, breaks = 10),data = Auto)
```

De las anteriores, el dato *displacement* sufre un repunte, en el intervalo (262,300]. Si nos fijamos realmente en los datos, esta sección tiene muy pocos datos, es normal que se pueda ver alterado. Salvo esto, las 3 muestran una tendencia clara. Si hubiera que elegir entre una de las tres, probablemente cogería *horsepower*, pues *weight* tiene muchos outliners y *displacement* tiene ese comportamiento un poco extraño comentado antes.

### b)

Por lo dicho anteriormente, seleccionaremos como posibles variables predictoras *displacement*, *horsepower* y *weight*, pues las nubes de puntos parecen reflejar una tendencia clara.

### c)

Para separar los datos en entrenamiento y test, vamos a usar un muestreo sin remplazamiento. En nuestro caso, como tenemos muchos datos, el comportamiento de este muestreo aleatorio debería cubrir bien todos los casos. En el caso de que no se tuvieran buenos resultados, sería necesario hacer este muestreo de manera estratificada.

```{r}
idx.train = sample(nrow(Auto),size = nrow(Auto)*0.8)
Auto.train = Auto[idx.train,c("mpg","displacement","horsepower","weight")]
Auto.test = Auto[-idx.train,c("mpg","displacement","horsepower","weight")]
```

### d)

Vamos a crear la variable mpg01. Para poder hacer el aprendizaje de modo correcto, la mediana la realizaremos sólo con los datos de entrenamiento, y dicha mediana la usaremos para ambos conjuntos.

```{r}
Auto.train = data.frame(mpg01=sign(Auto.train$mpg>=median(Auto.train$mpg))*2-1, Auto.train)
Auto.test = data.frame(mpg01=sign(Auto.test$mpg>=median(Auto.train$mpg))*2-1, Auto.test)
```

Una vez tenemos los datos formateados de la manera desada, procedemos a aplicar los distintos modelos que se nos piden.

#### Regresión logística

Para realizar la regresión logística, utilizamos la función *glm*.

```{r}
modelo.RegLog = glm(mpg01 ~ displacement + horsepower + weight, data = Auto.train, start=c(log(mean(Auto.train$mpg)),0,0,0))
```

Con el modelo ya aprendido, realizamos la predicción del test.

```{r}
prediccion.glm = predict(modelo.RegLog,newdata = Auto.test)
```

Ahora, definimos como error cuando el clasificador nos daría la clase incorrecta. Esto se produce cuando el signo de la predicción y el de *mpg01* son distintos.

```{r}
sum((sign(-prediccion.glm*Auto.test$mpg01)+1)/2)/nrow(Auto.test) * 100
```

Obtenemos un error bastante bajo, del 6 por ciento.

#### K-NN

Hacemos algo similar que lo realizado para la regresión logística, pero con la función *knn*.

```{r}
library(class)
prediccion.knn = knn(Auto.train[,c("displacement","horsepower","weight")],Auto.test[,c("displacement","horsepower","weight")], Auto.train[,"mpg01"],k = 3)

```

A diferencia de la regresión, los resultados del clasificador K-NN son directamente las clases. Luego para saber si hemos fallado, sólo tenemos que mirar si los resultados son distintos.
```{r}
sum(prediccion.knn != Auto.test$mpg01)/nrow(Auto.test) * 100
```

El error que encontramos está entorno al 8 por ciento.

Para intentar conocer el mejor parámetro de la variable *k*, vamos a realizar la prueba anterior cambiando los valores de k.

```{r}
for(k in 1:20){
  prediccion.knn = knn(Auto.train[,c("displacement","horsepower","weight")],Auto.test[,c("displacement","horsepower","weight")], Auto.train[,"mpg01"],k = k)
  error = sum(prediccion.knn != Auto.test$mpg01)/nrow(Auto.test) * 100
  cat(paste0("K: ",k, " \tError: ",error,"\n"))
}
```

Podemos ver, los valores que tienen menos error son 5 y 7. No tenemos más información, luego no podemos decidirnos entre los dos. 

R tiene una funcionalidad que, automáticamente nos devuelve el mejor parámetro *k*. Es la función *tune.knn()* de la librería *e1071*.
```{r}
#library(e1071)
#tune.values = tune.knn(x = Auto.train[,c("displacement","horsepower","weight")], y = Auto.train[,"mpg01"],k = 1:20)
#summary(tune.values)
```

Para poder realizar el ejercicio sobre la curva ROC, guardamos la predicción del K-NN, pero obteniendo la probabilidad de pertenencia a la clase, y no sólo la clase.

```{r}
prediccion.knn = knn(Auto.train[,c("displacement","horsepower","weight")],Auto.test[,c("displacement","horsepower","weight")], Auto.train[,"mpg01"],k = 5, prob = TRUE)
```


#### Curvas ROC
Para poder pintar las *curvas ROC*, tenemos que usar las ordenes del paquete *ROCR* *prediction* y *performance*.

Primero, pintamos la curva de la regresión logística.
```{r}
pred <- prediction(prediccion.glm, Auto.test$mpg01)
perf <- performance(pred, measure = "tpr", x.measure = "fpr") 
plot(perf, col=rainbow(10))
```

Después, pintamos la curva del clasificador K-NN
```{r}
pred <- prediction(as.double(prediccion.knn), Auto.test$mpg01)
perf <- performance(pred, measure = "tpr", x.measure = "fpr") 
plot(perf, col=rainbow(10))
```

### e)

Para poder estimar los dos modelos usando validación cruzada, tenemos que generar las 5 particiones. Para crear los *folds* o particiones que usaremos para la validación cruzada, podemos usar la función *createFolds* del paquete *caret*.
```{r}
library(caret)
folds = createFolds(1:nrow(Auto), k = 5)

error.log = vector("numeric",5)
error.knn = vector("numeric",5)

for(i in 1:5){
  Auto.train = Auto[-folds[[i]],c("mpg","displacement","horsepower","weight")]
  Auto.test = Auto[folds[[i]],c("mpg","displacement","horsepower","weight")]
  
  Auto.train = data.frame(mpg01=sign(Auto.train$mpg>=median(Auto.train$mpg))*2-1, Auto.train)
  Auto.test = data.frame(mpg01=sign(Auto.test$mpg>=median(Auto.train$mpg))*2-1, Auto.test)
  
  #Regresion logistica
  prediccion.glm = predict(modelo.RegLog,newdata = Auto.test)
  error.log[i] = sum((sign(-prediccion.glm*Auto.test$mpg01)+1)/2)/nrow(Auto.test) * 100
  #Clasificacion K-NN
  prediccion.knn = knn(Auto.train[,c("displacement","horsepower","weight")],Auto.test[,c("displacement","horsepower","weight")], Auto.train[,"mpg01"],k = k)
  error.knn[i] = sum(prediccion.knn != Auto.test$mpg01)/nrow(Auto.test) * 100
}

cat(paste0("Error medio cometido con regresión logística: ",mean(error.log)))
cat(paste0("Error medio cometido con K-NN: ",mean(error.knn)))
```


Podemos por tanto, confirmar lo visto en el apartado anterior. La regresión logística parece comportarse mejor que K-NN.

## Apartado 2

Vamos a usar un ajuste de tipo LASSO.
```{r}
#prediccion.lasso <- lars(x, y, type="lasso")
#summary(prediccion.lasso)
```

## Apartado 3

### a)
Para empezar, vamos a prepara las particiones de entrenamiento(80%) y de test(20%)

```{r}
library(MASS)
idx.train = sample(nrow(Boston),size = nrow(Boston)*0.8)
Boston.train = Boston[idx.train,]
Boston.test = Boston[-idx.train,]
```

Con los datos ya preparamos, vamos a realizar los ejercicios.

### b)
Bagging es un *random forest* donde podemos coger cualquier atributo para la generación de arboles.

```{r}
library(randomForest)
random.forest = randomForest(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat, data = Boston.train, mtry = 13)
pred = predict(random.forest,Boston.test)
error = sum(abs(Boston.test["medv"]-pred))/nrow(Boston.test)
cat(paste0("El error obtenido es: ",error))
```


### c)


```{r}
random.forest = randomForest(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat, data = Boston.train)
pred = predict(random.forest,Boston.test)
error = sum(abs(Boston.test["medv"]-pred))/nrow(Boston.test)
cat(paste0("El error obtenido es: ",error))
```

### d)

Vamos a ajustar un modelo de Boosting para poder comparar los resultados de los apartados anteriores.

```{r}
library(gbm)
boost = gbm(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat, data = Boston.train, distribution = "gaussian")
pred = predict(boost,Boston.test,n.trees = 100)
error = sum(abs(Boston.test["medv"]-pred))/nrow(Boston.test)
cat(paste0("El error obtenido es: ",error))
```

## Apartado 4

### a)

Primero, vamos a preparar los datos.

```{r}
idx.train = sample(nrow(OJ),size = 800)
OJ.train = OJ[idx.train,]
OJ.test = OJ[-idx.train,]
```

Vamos a entrenar el arbol de decisión.

```{r}
library(tree)
arbol = tree(Purchase~WeekofPurchase+StoreID+PriceCH+PriceMM+DiscCH+DiscMM+SpecialCH+SpecialMM+LoyalCH+SalePriceMM+SalePriceCH+PriceDiff+Store7+PctDiscMM+PctDiscCH+ListPriceDiff+STORE, data = OJ.train)
```

Con esto, ya habríamos ajustado nuestro árbol con los datos de entrenamiento.

### b)

Vamos a ver los resultados de la orden *summary*.

```{r}
summary(arbol)
```

### c)

```{r}
plot(arbol, uniform=TRUE, 
  	main="Árbol de clasificación")
text(arbol, use.n=TRUE, all=TRUE, cex=.8)
```

### d)

```{r}
probabilidad.prediccion = predict(arbol,OJ.test)
OJ.prediction= vector(length = nrow(probabilidad.prediccion))
OJ.prediction[probabilidad.prediccion[,1]>0.5] = "CH"
OJ.prediction[probabilidad.prediccion[,1]<=0.5] = "MM"
confusionMatrix(OJ.prediction,OJ.test$Purchase)
```

### e)

```{r}
cv.tree(arbol)
```
